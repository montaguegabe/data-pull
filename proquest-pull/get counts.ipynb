{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "with open('news_corpus.pickle', 'rb') as pf:\n",
    "\tarticles = pickle.load(pf)\n",
    "fdist = FreqDist()\n",
    "for article in articles:\n",
    "\tfor word in word_tokenize(article):\n",
    "\t\tfdist[word.lower()] += 1\n",
    "total_count = fdist.N()\n",
    "for word, count in fdist.items():\n",
    "\tvocab[word] = [count / float(total_count), 0]\n",
    "\n",
    "with open('scientific_corpus.pickle', 'rb') as pf:\n",
    "\tarticles = pickle.load(pf)\n",
    "fdist = FreqDist()\n",
    "for article in articles:\n",
    "\tfor word in word_tokenize(article):\n",
    "\t\tfdist[word.lower()] += 1\n",
    "total_count = fdist.N()\n",
    "for word, count in fdist.items():\n",
    "\told_entry = vocab[word] if word in vocab.keys() else [0, 0]\n",
    "\told_entry[1] = count / float(total_count)\n",
    "\tvocab[word] = old_entry\n",
    "\n",
    "def lam_fun(value):\n",
    "\treturn value[1][0] + value[1][1]\n",
    "sorted_words = sorted(vocab.items(), key=lam_fun, reverse=True)\n",
    "with open('counts.pickle', 'wb') as pf:\n",
    "\tpickle.dump(sorted_words, pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this section, I'm constructing something similar, but just using cross-comparisons\n",
    "#from the scientific corpus.\n",
    "with open('scientific_corpus.pickle', 'rb') as pf:\n",
    "    raw_articles = pickle.load(pf)\n",
    "shuffle(raw_articles)\n",
    "first_half = raw_articles[0:round(len(raw_articles) / 2)]\n",
    "second_half = raw_articles[round(len(raw_articles) / 2) :]\n",
    "vocab = {}\n",
    "\n",
    "fdist = FreqDist()\n",
    "for article in first_half:\n",
    "\tfor word in word_tokenize(article):\n",
    "\t\tfdist[word.lower()] += 1\n",
    "total_count = fdist.N()\n",
    "for word, count in fdist.items():\n",
    "\tvocab[word] = [count / float(total_count), 0]\n",
    "\n",
    "for article in second_half:\n",
    "\tfor word in word_tokenize(article):\n",
    "\t\tfdist[word.lower()] += 1\n",
    "total_count = fdist.N()\n",
    "for word, count in fdist.items():\n",
    "\told_entry = vocab[word] if word in vocab.keys() else [0, 0]\n",
    "\told_entry[1] = count / float(total_count)\n",
    "\tvocab[word] = old_entry\n",
    "\n",
    "def lam_fun(value):\n",
    "\treturn value[1][0] + value[1][1]\n",
    "sorted_words = sorted(vocab.items(), key=lam_fun, reverse=True)\n",
    "with open('sci_v_sci_counts.pickle', 'wb') as pf:\n",
    "\tpickle.dump(sorted_words, pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same, but for newspaper corpus.\n",
    "with open('news_corpus.pickle', 'rb') as pf:\n",
    "    raw_articles = pickle.load(pf)\n",
    "shuffle(raw_articles)\n",
    "first_half = raw_articles[0:round(len(raw_articles) / 2)]\n",
    "second_half = raw_articles[round(len(raw_articles) / 2) :]\n",
    "vocab = {}\n",
    "\n",
    "fdist = FreqDist()\n",
    "for article in first_half:\n",
    "\tfor word in word_tokenize(article):\n",
    "\t\tfdist[word.lower()] += 1\n",
    "total_count = fdist.N()\n",
    "for word, count in fdist.items():\n",
    "\tvocab[word] = [count / float(total_count), 0]\n",
    "\n",
    "for article in second_half:\n",
    "\tfor word in word_tokenize(article):\n",
    "\t\tfdist[word.lower()] += 1\n",
    "total_count = fdist.N()\n",
    "for word, count in fdist.items():\n",
    "\told_entry = vocab[word] if word in vocab.keys() else [0, 0]\n",
    "\told_entry[1] = count / float(total_count)\n",
    "\tvocab[word] = old_entry\n",
    "\n",
    "def lam_fun(value):\n",
    "\treturn value[1][0] + value[1][1]\n",
    "sorted_words = sorted(vocab.items(), key=lam_fun, reverse=True)\n",
    "with open('nvn_counts.pickle', 'wb') as pf:\n",
    "\tpickle.dump(sorted_words, pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "rankings = {}\n",
    "with open('news_corpus.pickle', 'rb') as pf:\n",
    "\tarticles = pickle.load(pf)\n",
    "fdist = FreqDist()\n",
    "for article in articles:\n",
    "\tfor word in word_tokenize(article):\n",
    "\t\tfdist[word.lower()] += 1\n",
    "total_count = fdist.N()\n",
    "for word, count in fdist.items():\n",
    "\tvocab[word] = [count / float(total_count), 0]\n",
    "ordered_words = sorted(fdist.items(), key= lambda value: value[1], reverse=True)\n",
    "counter = len(ordered_words)\n",
    "for word, count in ordered_words:\n",
    "    vocab[word] = [count / float(total_count), 0]\n",
    "    rankings[word] = [counter, 0]\n",
    "    counter -= 1\n",
    "    \n",
    "with open('scientific_corpus.pickle', 'rb') as pf:\n",
    "\tarticles = pickle.load(pf)\n",
    "fdist = FreqDist()\n",
    "for article in articles:\n",
    "\tfor word in word_tokenize(article):\n",
    "\t\tfdist[word.lower()] += 1\n",
    "total_count = fdist.N()\n",
    "ordered_words = sorted(fdist.items(), key= lambda value: value[1], reverse=True)\n",
    "counter = len(ordered_words)\n",
    "for word, count in ordered_words:\n",
    "    old_entry = vocab[word] if word in vocab.keys() else [0, 0]\n",
    "    old_entry[1] = count / float(total_count)\n",
    "    old_ranking = rankings[word] if word in rankings.keys() else [0, 0]\n",
    "    old_ranking[1] = counter\n",
    "    vocab[word] = old_entry\n",
    "    rankings[word] = old_ranking\n",
    "    counter -= 1\n",
    "\n",
    "def lam_fun(value):\n",
    "\treturn value[1][0] + value[1][1]\n",
    "sorted_words = sorted(vocab.items(), key=lam_fun, reverse=True)\n",
    "with open('counts.pickle', 'wb') as pf:\n",
    "    pickle.dump(sorted_words, pf)\n",
    "with open('rankings.pickle', 'wb') as pf:\n",
    "\tpickle.dump(rankings, pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

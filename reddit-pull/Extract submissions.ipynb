{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import csv\n",
    "import json\n",
    "import wget\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RS_2011-12.bz2', 'RS_2009-02.bz2', 'RS_2012-07.bz2', 'RS_2008-06.bz2', 'RS_2016-01.bz2', 'RS_2009-06.bz2', 'RS_2010-11.bz2', 'RS_2016-08.bz2', 'RS_2008-10.bz2', 'RS_2008-05.bz2', 'RS_2014-10.bz2', 'RS_2006-03.bz2', 'RS_2013-12.bz2', 'RS_2008-01.bz2', 'RS_2016-05.bz2', 'RS_2007-03.bz2', 'RS_2011-07.bz2', 'RS_2009-01.bz2', 'RS_2010-02.bz2', 'RS_2008-02.bz2', 'RS_2014-02.bz2', 'RS_2013-11.bz2', 'RS_2012-09.bz2', 'RS_2010-06.bz2', 'RS_2011-09.bz2', 'RS_2012-05.bz2', 'RS_2015-01.bz2', 'RS_2014-05.bz2', 'RS_2014-08.bz2', 'RS_2015-06.bz2', 'RS_2006-07.bz2', 'RS_2007-01.bz2', 'RS_2009-09.bz2', 'RS_2010-10.bz2', 'RS_2016-02.bz2', 'RS_2015-04.bz2', 'RS_2009-12.bz2', 'RS_2008-12.bz2', 'RS_2008-04.bz2', 'RS_2012-12.bz2', 'RS_2009-07.bz2', 'RS_2011-01.bz2', 'RS_2013-09.bz2', 'RS_2016-07.bz2', 'RS_2015-05.bz2', 'RS_2014-06.bz2', 'RS_2014-09.bz2', 'RS_2014-11.bz2', 'RS_2016-06.bz2', 'RS_2016-04.bz2', 'RS_2011-11.bz2', 'RS_2009-04.bz2', 'RS_2010-03.bz2', 'RS_2014-07.bz2', 'RS_2014-04.bz2', 'RS_2015-09.bz2', 'RS_2015-02.bz2', 'RS_2014-01.bz2', 'RS_2008-07.bz2', 'RS_2012-10.bz2', 'RS_2006-05.bz2', 'RS_2009-08.bz2', 'RS_2014-03.bz2', 'RS_2012-06.bz2', 'RS_2011-08.bz2', 'RS_2011-10.bz2', 'RS_2009-11.bz2', 'RS_2011-04.bz2', 'RS_2013-06.bz2', 'RS_2007-07.bz2', 'RS_2011-03.bz2', 'RS_2012-01.bz2', 'RS_2013-10.bz2', 'RS_2014-12.bz2', 'RS_2007-12.bz2', 'RS_2012-08.bz2', 'RS_2007-02.bz2', 'RS_2011-05.bz2', 'RS_2007-11.bz2', 'RS_2010-05.bz2', 'RS_2010-07.bz2', 'RS_2012-02.bz2', 'RS_2013-07.bz2', 'RS_2013-05.bz2', 'RS_2006-06.bz2', 'RS_2015-08.bz2', 'RS_2007-10.bz2', 'RS_2010-08.bz2', 'RS_2015-07.bz2', 'RS_2015-12.bz2', 'RS_2015-03.bz2', 'RS_2010-09.bz2', 'RS_2009-05.bz2', 'RS_2011-02.bz2', 'RS_2006-02.bz2', 'RS_2006-01.bz2', 'RS_2011-06.bz2', 'RS_2012-04.bz2', 'RS_2010-12.bz2', 'RS_2012-03.bz2', 'RS_2013-02.bz2', 'RS_2013-01.bz2', 'RS_2009-03.bz2', 'RS_2009-10.bz2', 'RS_2012-11.bz2', 'RS_2015-11.bz2', 'RS_2008-03.bz2', 'RS_2013-08.bz2', 'RS_2016-03.bz2', 'RS_2007-06.bz2', 'RS_2006-04.bz2', 'RS_2013-04.bz2', 'RS_2008-08.bz2', 'RS_2008-09.bz2', 'RS_2013-03.bz2', 'RS_2015-10.bz2', 'RS_2010-01.bz2', 'RS_2008-11.bz2', 'RS_2010-04.bz2'}\n"
     ]
    }
   ],
   "source": [
    "with open('filenames.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    filenames = []\n",
    "    for row in reader:\n",
    "        filenames.append(row['Filename'])\n",
    "if os.path.isfile('visited.pickle'):\n",
    "    with open('visited.pickle', 'rb') as pfile:\n",
    "        visited = pickle.load(pfile)\n",
    "else:\n",
    "    visited = set()\n",
    "print(visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading RS_2015-10.bz2\n",
      "   Decompressing\n",
      "   Finding askscience\n",
      "Finished RS_2015-10.bz2\n",
      "Downloading RS_2015-11.bz2\n",
      "   Decompressing\n",
      "   Finding askscience\n",
      "Finished RS_2015-11.bz2\n",
      "Downloading RS_2015-12.bz2\n",
      "   Decompressing\n",
      "   Finding askscience\n",
      "Finished RS_2015-12.bz2\n",
      "Downloading RS_2016-01.bz2\n"
     ]
    }
   ],
   "source": [
    "# For file in filenames:\n",
    "import urllib.request\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.167 Safari/537.36')]\n",
    "urllib.request.install_opener(opener)\n",
    "for name in filenames:\n",
    "    if name in visited: continue\n",
    "    print(\"Downloading \" + name)\n",
    "    urllib.request.urlretrieve(\"https://files.pushshift.io/reddit/submissions/\" + name, name)\n",
    "    print(\"   Decompressing\")\n",
    "    with open(\"decompressed.bin\", 'wb') as new_file, bz2.BZ2File(name, 'rb') as file:\n",
    "        for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "            new_file.write(data)\n",
    "    print(\"   Finding askscience\")\n",
    "    with open(\"decompressed.bin\", 'r', encoding='utf-8') as file, open('raw_submissions.txt', \"a+\", encoding='utf-8') as outfile:\n",
    "        towrite = \"\"\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "            except ValueError:\n",
    "                print(\"JSON decoding failed\")\n",
    "                continue\n",
    "            if 'subreddit' not in entry: continue\n",
    "            if entry['subreddit'].lower() == 'askscience' or \\\n",
    "                (entry['subreddit'].lower() == 'science' and 'askscience' in entry['title'].lower()):\n",
    "                towrite = towrite + line + \"\\n\"\n",
    "        outfile.write(towrite)\n",
    "    os.remove(name)\n",
    "    visited.add(name)\n",
    "    with open('visited.pickle', 'wb') as pfile:\n",
    "        pickle.dump(visited, pfile)\n",
    "    print(\"Finished \" + name)\n",
    "#     break\n",
    "# I'll need to clean up the file - remove duplicates, remove non-climate change topics, remove blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RS_2016-09.bz2', 'RS_2016-10.bz2', 'RS_2016-11.bz2', 'RS_2016-12.bz2', 'RS_2017-01.bz2', 'RS_2017-02.bz2', 'RS_2017-03.bz2', 'RS_2017-04.bz2', 'RS_2017-05.bz2', 'RS_2017-06.bz2', 'RS_2017-07.bz2', 'RS_2017-08.bz2', 'RS_2017-09.bz2', 'RS_2017-10.bz2', 'RS_2017-11.bz2']\n",
      "Downloading RS_2016-09.bz2Downloading RS_2016-10.bz2\n",
      "\n",
      "Downloading RS_2016-11.bz2Downloading RS_2016-12.bz2\n",
      "\n",
      "Downloading RS_2017-01.bz2\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import urllib.request\n",
    "\n",
    "def load_url(name):\n",
    "    print(\"Downloading \" + name)\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.167 Safari/537.36')]\n",
    "    urllib.request.install_opener(opener)\n",
    "    urllib.request.urlretrieve(\"https://files.pushshift.io/reddit/submissions/\" + name, name)\n",
    "    return(name)\n",
    "\n",
    "to_visit = []\n",
    "for file in filenames:\n",
    "    if file not in visited:\n",
    "        to_visit.append(file)\n",
    "print(to_visit)\n",
    "\n",
    "downloaded = []\n",
    "\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(5) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_url = {executor.submit(load_url, name): name for name in to_visit}\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            name = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (url, exc))\n",
    "        else:\n",
    "            print(\"Finished downloading \" + name)\n",
    "            downloaded.append(name)\n",
    "            with open('downloaded.pickle', 'wb') as pfile:\n",
    "                pickle.dump(downloaded, pfile)\n",
    "for name in downloaded:\n",
    "    with open(\"decompressed.bin\", 'wb') as new_file, bz2.BZ2File(name, 'rb') as file:\n",
    "        for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "            new_file.write(data)\n",
    "    print(\"   Finding askscience\")\n",
    "    with open(\"decompressed.bin\", 'r', encoding='utf-8') as file, open('raw_submissions.txt', \"a+\", encoding='utf-8') as outfile:\n",
    "        towrite = \"\"\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "            except ValueError:\n",
    "                print(\"JSON decoding failed\")\n",
    "                continue\n",
    "            if 'subreddit' not in entry: continue\n",
    "            if entry['subreddit'].lower() == 'askscience' or \\\n",
    "                (entry['subreddit'].lower() == 'science' and 'askscience' in entry['title'].lower()):\n",
    "                towrite = towrite + line + \"\\n\"\n",
    "        outfile.write(towrite)\n",
    "    os.remove(name)\n",
    "    visited.add(name)\n",
    "    with open('visited.pickle', 'wb') as pfile:\n",
    "        pickle.dump(visited, pfile)\n",
    "    print(\"Finished \" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

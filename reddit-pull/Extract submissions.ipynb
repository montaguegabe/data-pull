{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filenames.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    filenames = []\n",
    "    for row in reader:\n",
    "        filenames.append(row['Filename'])\n",
    "if os.path.isfile('visited.pickle'):\n",
    "    with open('visited.pickle', 'rb') as pfile:\n",
    "        visited = pickle.load(pfile)\n",
    "else:\n",
    "    visited = set()\n",
    "# print(visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RS_2016-09.bz2', 'RS_2016-10.bz2', 'RS_2016-11.bz2', 'RS_2016-12.bz2', 'RS_2017-01.bz2']\n",
      "['RS_2017-02.bz2', 'RS_2017-05.bz2', 'RS_2017-06.bz2', 'RS_2017-04.bz2', 'RS_2017-03.bz2', 'RS_2017-07.bz2', 'RS_2017-09.bz2', 'RS_2017-08.bz2', 'RS_2017-10.bz2', 'RS_2017-11.bz2']\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('downloaded.pickle'):\n",
    "    with open('downloaded.pickle', 'rb') as pfile:\n",
    "        downloaded = pickle.load(pfile)\n",
    "else:\n",
    "    downloaded = []\n",
    "\n",
    "to_visit = []\n",
    "for file in filenames:\n",
    "    if file not in visited:\n",
    "        if file not in downloaded:\n",
    "            to_visit.append(file)\n",
    "print(to_visit)\n",
    "print(downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading RS_2016-09.bz2\n",
      "Downloading RS_2016-10.bz2\n",
      "Downloading RS_2016-11.bz2\n",
      "Downloading RS_2016-12.bz2\n",
      "Downloading RS_2017-01.bz2\n",
      "Finished downloading RS_2016-09.bz2\n",
      "Finished downloading RS_2016-10.bz2\n",
      "Finished downloading RS_2016-11.bz2\n",
      "Finished downloading RS_2016-12.bz2\n",
      "Finished downloading RS_2017-01.bz2\n"
     ]
    }
   ],
   "source": [
    "def load_url(name):\n",
    "    print(\"Downloading \" + name)\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.167 Safari/537.36')]\n",
    "    urllib.request.install_opener(opener)\n",
    "    urllib.request.urlretrieve(\"https://files.pushshift.io/reddit/submissions/\" + name, name)\n",
    "    return(name)\n",
    "\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(5) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_url = {executor.submit(load_url, name): name for name in to_visit}\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            name = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (url, exc))\n",
    "        else:\n",
    "            print(\"Finished downloading \" + name)\n",
    "            downloaded.append(name)\n",
    "            with open('downloaded.pickle', 'wb') as pfile:\n",
    "                pickle.dump(downloaded, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RS_2017-02.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-02.bz2\n",
      "Starting RS_2017-05.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-05.bz2\n",
      "Starting RS_2017-06.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-06.bz2\n",
      "Starting RS_2017-04.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-04.bz2\n",
      "Starting RS_2017-03.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-03.bz2\n",
      "Starting RS_2017-07.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-07.bz2\n",
      "Starting RS_2017-09.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-09.bz2\n",
      "Starting RS_2017-08.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-08.bz2\n",
      "Starting RS_2017-10.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-10.bz2\n",
      "Starting RS_2017-11.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-11.bz2\n",
      "Starting RS_2016-09.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2016-09.bz2\n",
      "Starting RS_2016-10.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2016-10.bz2\n",
      "Starting RS_2016-11.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2016-11.bz2\n",
      "Starting RS_2016-12.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2016-12.bz2\n",
      "Starting RS_2017-01.bz2\n",
      "   Finding askscience\n",
      "Finished RS_2017-01.bz2\n"
     ]
    }
   ],
   "source": [
    "for name in downloaded:\n",
    "    print(\"Starting \" + name)\n",
    "    with open(\"decompressed.bin\", 'wb') as new_file, bz2.BZ2File(name, 'rb') as file:\n",
    "        for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "            new_file.write(data)\n",
    "    print(\"   Finding askscience\")\n",
    "    with open(\"decompressed.bin\", 'r', encoding='utf-8') as file, open('raw_submissions.txt', \"a+\", encoding='utf-8') as outfile:\n",
    "        towrite = \"\"\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "            except ValueError:\n",
    "                print(\"JSON decoding failed\")\n",
    "                continue\n",
    "            if 'subreddit' not in entry: continue\n",
    "            if entry['subreddit'].lower() == 'askscience' or \\\n",
    "                (entry['subreddit'].lower() == 'science' and 'askscience' in entry['title'].lower()):\n",
    "                towrite = towrite + line + \"\\n\"\n",
    "        outfile.write(towrite)\n",
    "    os.remove(name)\n",
    "    visited.add(name)\n",
    "    with open('visited.pickle', 'wb') as pfile:\n",
    "        pickle.dump(visited, pfile)\n",
    "    print(\"Finished \" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single threaded download version\n",
    "# For file in filenames:\n",
    "# import urllib.request\n",
    "# opener = urllib.request.build_opener()\n",
    "# opener.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.167 Safari/537.36')]\n",
    "# urllib.request.install_opener(opener)\n",
    "# for name in filenames:\n",
    "#     if name in visited: continue\n",
    "#     print(\"Downloading \" + name)\n",
    "#     urllib.request.urlretrieve(\"https://files.pushshift.io/reddit/submissions/\" + name, name)\n",
    "#     print(\"   Decompressing\")\n",
    "#     with open(\"decompressed.bin\", 'wb') as new_file, bz2.BZ2File(name, 'rb') as file:\n",
    "#         for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "#             new_file.write(data)\n",
    "#     print(\"   Finding askscience\")\n",
    "#     with open(\"decompressed.bin\", 'r', encoding='utf-8') as file, open('raw_submissions.txt', \"a+\", encoding='utf-8') as outfile:\n",
    "#         towrite = \"\"\n",
    "#         for line in file:\n",
    "#             try:\n",
    "#                 entry = json.loads(line)\n",
    "#             except ValueError:\n",
    "#                 print(\"JSON decoding failed\")\n",
    "#                 continue\n",
    "#             if 'subreddit' not in entry: continue\n",
    "#             if entry['subreddit'].lower() == 'askscience' or \\\n",
    "#                 (entry['subreddit'].lower() == 'science' and 'askscience' in entry['title'].lower()):\n",
    "#                 towrite = towrite + line + \"\\n\"\n",
    "#         outfile.write(towrite)\n",
    "#     os.remove(name)\n",
    "#     visited.add(name)\n",
    "#     with open('visited.pickle', 'wb') as pfile:\n",
    "#         pickle.dump(visited, pfile)\n",
    "#     print(\"Finished \" + name)\n",
    "# #     break\n",
    "# # I'll need to clean up the file - remove duplicates, remove non-climate change topics, remove blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import logging, gensim, json\n",
    "from gensim.models import word2vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decoding failed. {\"downs\":4,\"link_flair_text\":null,\"distinguished\":null,\"media\":null,\"url\":\"http://www.reddit.com/r/askscience/comments/ccamo/can_all_of_math_be_compressed_into_a_short_list/\",\"link_flair_css_class\":null,\"id\":\"ccamo\",\"edited\":true,\"num_reports\":null,\"created_utc\":1275913150,\"banned_by\":null,\"name\":\"t3_ccamo\",\"subreddit\":\"askscience\",\"title\":\"can all of math be compressed into a short list of equations?\",\"author_flair_text\":null,\"is_self\":true,\"author\":\"Enieublis_McShlaggrm\",\"media_embed\":{},\"permalink\":\"/r/askscience/comments/ccamo/can_all_of_math_be_compressed_into_a_short_list/\",\"author_flair_css_class\":null,\"selftext\":\"So far I'm through calculus one.\\n\\nall of the math before me seems to be derivable from:\\n\\nlim h to 0 (f(x+h)-f(x))/h\\n         -all of calculus\\n         -all geometry\\n\\na^2+b^2=c^2\\n         -sin (angle)= opp/hyp\\n         -cos (angle)= adj/hyp\\n         -all trigonometry\\n\\narithmetic(1+1=10, 1+0=1, 0+0=0, 1-1=0, 1-0=1, 1x1=1, 1x0=0, 0/1=0, 1/0=  {\"downs\":1,\"link_flair_text\":null,\"distinguished\":null,\"media\":null,\"url\":\"http://www.reddit.com/r/askscience/comments/caguh/how_can_a_solid_object_like_a_mirror_retain_a/\",\"link_flair_css_class\":null,\"id\":\"caguh\",\"edited\":false,\"num_reports\":null,\"created_utc\":1275446508,\"banned_by\":null,\"name\":\"t3_caguh\",\"subreddit\":\"askscience\",\"title\":\"How can a solid object, like a mirror, retain a smell?\",\"author_flair_text\":null,\"is_self\":true,\"author\":\"mkgr4boski\",\"media_embed\":{},\"permalink\":\"/r/askscience/comments/caguh/how_can_a_solid_object_like_a_mirror_retain_a/\",\"author_flair_css_class\":null,\"selftext\":\"i keep a box of incense in a closed drawer with various items in it. today, i took out an old cell phone that was in the drawer to give away, and it smelled like incense! and the smell didn't dissipate over the course of the day. then just now i took out a mirror from the drawer, and it smelled too. can anyone explain?\",\"domain\":\"self.askscience\",\"num_comments\":2,\"likes\":null,\"clicked\":false,\"thumbnail\":\"self\",\"saved\":false,\"subreddit_id\":\"t5_2qm4e\",\"ups\":4,\"approved_by\":null,\"score\":3,\"selftext_html\":\"&lt;!-- SC_OFF --&gt;&lt;div class=\\\"md\\\"&gt;&lt;p&gt;i keep a box of incense in a closed drawer with various items in it. today, i took out an old cell phone that was in the drawer to give away, and it smelled like incense! and the smell didn&amp;#39;t dissipate over the course of the day. then just now i took out a mirror from the drawer, and it smelled too. can anyone explain?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;\",\"created\":1275450108,\"hidden\":false,\"over_18\":false}\n",
      "\n",
      "JSON decoding failed. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "titles = []\n",
    "# Extract all title text from all articles in the uncompressed AskReddit file\n",
    "\n",
    "with open('processed_submissions.txt','r', encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "        except ValueError:\n",
    "            print(\"JSON decoding failed. \" + line)\n",
    "            continue\n",
    "        title = entry['title'].lower()\n",
    "        tokens = nltk.word_tokenize(title)\n",
    "        titles.append(tokens)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-05 16:24:02,193 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-03-05 16:24:02,201 : INFO : collecting all words and their counts\n",
      "2018-03-05 16:24:02,204 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-05 16:24:02,290 : INFO : PROGRESS: at sentence #10000, processed 146843 words, keeping 12738 word types\n",
      "2018-03-05 16:24:02,327 : INFO : PROGRESS: at sentence #20000, processed 296698 words, keeping 18618 word types\n",
      "2018-03-05 16:24:02,391 : INFO : PROGRESS: at sentence #30000, processed 448897 words, keeping 23170 word types\n",
      "2018-03-05 16:24:02,449 : INFO : PROGRESS: at sentence #40000, processed 595174 words, keeping 27120 word types\n",
      "2018-03-05 16:24:02,487 : INFO : PROGRESS: at sentence #50000, processed 742527 words, keeping 30626 word types\n",
      "2018-03-05 16:24:02,552 : INFO : PROGRESS: at sentence #60000, processed 893497 words, keeping 33497 word types\n",
      "2018-03-05 16:24:02,606 : INFO : PROGRESS: at sentence #70000, processed 1047195 words, keeping 36383 word types\n",
      "2018-03-05 16:24:02,666 : INFO : PROGRESS: at sentence #80000, processed 1197192 words, keeping 39000 word types\n",
      "2018-03-05 16:24:02,722 : INFO : PROGRESS: at sentence #90000, processed 1350836 words, keeping 41463 word types\n",
      "2018-03-05 16:24:02,765 : INFO : PROGRESS: at sentence #100000, processed 1503881 words, keeping 43741 word types\n",
      "2018-03-05 16:24:02,831 : INFO : PROGRESS: at sentence #110000, processed 1661332 words, keeping 45961 word types\n",
      "2018-03-05 16:24:02,873 : INFO : PROGRESS: at sentence #120000, processed 1821072 words, keeping 48135 word types\n",
      "2018-03-05 16:24:02,932 : INFO : PROGRESS: at sentence #130000, processed 1982671 words, keeping 50277 word types\n",
      "2018-03-05 16:24:02,998 : INFO : PROGRESS: at sentence #140000, processed 2143811 words, keeping 52348 word types\n",
      "2018-03-05 16:24:03,047 : INFO : PROGRESS: at sentence #150000, processed 2307297 words, keeping 54385 word types\n",
      "2018-03-05 16:24:03,122 : INFO : PROGRESS: at sentence #160000, processed 2477934 words, keeping 56520 word types\n",
      "2018-03-05 16:24:03,171 : INFO : PROGRESS: at sentence #170000, processed 2652205 words, keeping 58616 word types\n",
      "2018-03-05 16:24:03,237 : INFO : PROGRESS: at sentence #180000, processed 2825710 words, keeping 60642 word types\n",
      "2018-03-05 16:24:03,293 : INFO : PROGRESS: at sentence #190000, processed 2998228 words, keeping 62711 word types\n",
      "2018-03-05 16:24:03,360 : INFO : PROGRESS: at sentence #200000, processed 3170979 words, keeping 64604 word types\n",
      "2018-03-05 16:24:03,421 : INFO : PROGRESS: at sentence #210000, processed 3342796 words, keeping 66373 word types\n",
      "2018-03-05 16:24:03,489 : INFO : PROGRESS: at sentence #220000, processed 3513244 words, keeping 68358 word types\n",
      "2018-03-05 16:24:03,538 : INFO : PROGRESS: at sentence #230000, processed 3679125 words, keeping 70177 word types\n",
      "2018-03-05 16:24:03,608 : INFO : PROGRESS: at sentence #240000, processed 3844464 words, keeping 72025 word types\n",
      "2018-03-05 16:24:03,665 : INFO : PROGRESS: at sentence #250000, processed 4010689 words, keeping 73795 word types\n",
      "2018-03-05 16:24:03,723 : INFO : PROGRESS: at sentence #260000, processed 4177760 words, keeping 75564 word types\n",
      "2018-03-05 16:24:03,793 : INFO : PROGRESS: at sentence #270000, processed 4344860 words, keeping 77241 word types\n",
      "2018-03-05 16:24:03,839 : INFO : PROGRESS: at sentence #280000, processed 4509847 words, keeping 79005 word types\n",
      "2018-03-05 16:24:03,904 : INFO : PROGRESS: at sentence #290000, processed 4671069 words, keeping 80664 word types\n",
      "2018-03-05 16:24:03,959 : INFO : PROGRESS: at sentence #300000, processed 4836471 words, keeping 82355 word types\n",
      "2018-03-05 16:24:04,022 : INFO : PROGRESS: at sentence #310000, processed 5000928 words, keeping 84015 word types\n",
      "2018-03-05 16:24:04,077 : INFO : PROGRESS: at sentence #320000, processed 5160286 words, keeping 85687 word types\n",
      "2018-03-05 16:24:04,134 : INFO : PROGRESS: at sentence #330000, processed 5326513 words, keeping 87355 word types\n",
      "2018-03-05 16:24:04,221 : INFO : PROGRESS: at sentence #340000, processed 5492444 words, keeping 88915 word types\n",
      "2018-03-05 16:24:04,267 : INFO : PROGRESS: at sentence #350000, processed 5656630 words, keeping 90533 word types\n",
      "2018-03-05 16:24:04,337 : INFO : PROGRESS: at sentence #360000, processed 5818096 words, keeping 92085 word types\n",
      "2018-03-05 16:24:04,399 : INFO : PROGRESS: at sentence #370000, processed 5983926 words, keeping 93526 word types\n",
      "2018-03-05 16:24:04,519 : INFO : PROGRESS: at sentence #380000, processed 6148781 words, keeping 94942 word types\n",
      "2018-03-05 16:24:04,628 : INFO : PROGRESS: at sentence #390000, processed 6314475 words, keeping 96446 word types\n",
      "2018-03-05 16:24:04,696 : INFO : PROGRESS: at sentence #400000, processed 6480851 words, keeping 97840 word types\n",
      "2018-03-05 16:24:04,755 : INFO : PROGRESS: at sentence #410000, processed 6648387 words, keeping 99298 word types\n",
      "2018-03-05 16:24:04,828 : INFO : PROGRESS: at sentence #420000, processed 6814845 words, keeping 100756 word types\n",
      "2018-03-05 16:24:04,874 : INFO : PROGRESS: at sentence #430000, processed 6981813 words, keeping 102153 word types\n",
      "2018-03-05 16:24:04,949 : INFO : PROGRESS: at sentence #440000, processed 7148735 words, keeping 103573 word types\n",
      "2018-03-05 16:24:04,995 : INFO : PROGRESS: at sentence #450000, processed 7316029 words, keeping 105046 word types\n",
      "2018-03-05 16:24:05,060 : INFO : PROGRESS: at sentence #460000, processed 7482446 words, keeping 106609 word types\n",
      "2018-03-05 16:24:05,132 : INFO : PROGRESS: at sentence #470000, processed 7645794 words, keeping 108132 word types\n",
      "2018-03-05 16:24:05,197 : INFO : PROGRESS: at sentence #480000, processed 7814070 words, keeping 109614 word types\n",
      "2018-03-05 16:24:05,265 : INFO : PROGRESS: at sentence #490000, processed 7983723 words, keeping 111205 word types\n",
      "2018-03-05 16:24:05,344 : INFO : PROGRESS: at sentence #500000, processed 8151884 words, keeping 112611 word types\n",
      "2018-03-05 16:24:05,389 : INFO : PROGRESS: at sentence #510000, processed 8319065 words, keeping 113998 word types\n",
      "2018-03-05 16:24:05,456 : INFO : PROGRESS: at sentence #520000, processed 8488382 words, keeping 115392 word types\n",
      "2018-03-05 16:24:05,513 : INFO : PROGRESS: at sentence #530000, processed 8660353 words, keeping 116921 word types\n",
      "2018-03-05 16:24:05,585 : INFO : PROGRESS: at sentence #540000, processed 8830423 words, keeping 118390 word types\n",
      "2018-03-05 16:24:05,633 : INFO : PROGRESS: at sentence #550000, processed 8998560 words, keeping 119844 word types\n",
      "2018-03-05 16:24:05,703 : INFO : PROGRESS: at sentence #560000, processed 9168720 words, keeping 121361 word types\n",
      "2018-03-05 16:24:05,775 : INFO : PROGRESS: at sentence #570000, processed 9338386 words, keeping 122817 word types\n",
      "2018-03-05 16:24:05,840 : INFO : PROGRESS: at sentence #580000, processed 9506940 words, keeping 124421 word types\n",
      "2018-03-05 16:24:05,889 : INFO : PROGRESS: at sentence #590000, processed 9677493 words, keeping 125761 word types\n",
      "2018-03-05 16:24:05,954 : INFO : PROGRESS: at sentence #600000, processed 9848003 words, keeping 127262 word types\n",
      "2018-03-05 16:24:06,047 : INFO : PROGRESS: at sentence #610000, processed 10015108 words, keeping 128694 word types\n",
      "2018-03-05 16:24:06,136 : INFO : PROGRESS: at sentence #620000, processed 10182120 words, keeping 130187 word types\n",
      "2018-03-05 16:24:06,217 : INFO : PROGRESS: at sentence #630000, processed 10350058 words, keeping 131715 word types\n",
      "2018-03-05 16:24:06,264 : INFO : PROGRESS: at sentence #640000, processed 10519027 words, keeping 133261 word types\n",
      "2018-03-05 16:24:06,340 : INFO : PROGRESS: at sentence #650000, processed 10684949 words, keeping 134763 word types\n",
      "2018-03-05 16:24:06,411 : INFO : PROGRESS: at sentence #660000, processed 10853910 words, keeping 136238 word types\n",
      "2018-03-05 16:24:06,465 : INFO : PROGRESS: at sentence #670000, processed 11021333 words, keeping 137754 word types\n",
      "2018-03-05 16:24:06,572 : INFO : PROGRESS: at sentence #680000, processed 11191103 words, keeping 139262 word types\n",
      "2018-03-05 16:24:06,617 : INFO : PROGRESS: at sentence #690000, processed 11358116 words, keeping 140742 word types\n",
      "2018-03-05 16:24:06,688 : INFO : PROGRESS: at sentence #700000, processed 11525174 words, keeping 142193 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-05 16:24:06,736 : INFO : PROGRESS: at sentence #710000, processed 11690069 words, keeping 143674 word types\n",
      "2018-03-05 16:24:06,797 : INFO : PROGRESS: at sentence #720000, processed 11854665 words, keeping 145191 word types\n",
      "2018-03-05 16:24:06,883 : INFO : PROGRESS: at sentence #730000, processed 12019973 words, keeping 146608 word types\n",
      "2018-03-05 16:24:06,928 : INFO : PROGRESS: at sentence #740000, processed 12187002 words, keeping 148093 word types\n",
      "2018-03-05 16:24:06,990 : INFO : PROGRESS: at sentence #750000, processed 12354025 words, keeping 149347 word types\n",
      "2018-03-05 16:24:07,059 : INFO : PROGRESS: at sentence #760000, processed 12518313 words, keeping 150624 word types\n",
      "2018-03-05 16:24:07,118 : INFO : PROGRESS: at sentence #770000, processed 12682165 words, keeping 151859 word types\n",
      "2018-03-05 16:24:07,178 : INFO : PROGRESS: at sentence #780000, processed 12846688 words, keeping 153130 word types\n",
      "2018-03-05 16:24:07,222 : INFO : PROGRESS: at sentence #790000, processed 13012688 words, keeping 154310 word types\n",
      "2018-03-05 16:24:07,290 : INFO : PROGRESS: at sentence #800000, processed 13175594 words, keeping 155691 word types\n",
      "2018-03-05 16:24:07,355 : INFO : PROGRESS: at sentence #810000, processed 13341013 words, keeping 156926 word types\n",
      "2018-03-05 16:24:07,420 : INFO : PROGRESS: at sentence #820000, processed 13506345 words, keeping 158215 word types\n",
      "2018-03-05 16:24:07,466 : INFO : PROGRESS: at sentence #830000, processed 13674941 words, keeping 159279 word types\n",
      "2018-03-05 16:24:07,525 : INFO : PROGRESS: at sentence #840000, processed 13840471 words, keeping 160409 word types\n",
      "2018-03-05 16:24:07,580 : INFO : PROGRESS: at sentence #850000, processed 14005539 words, keeping 161545 word types\n",
      "2018-03-05 16:24:07,651 : INFO : PROGRESS: at sentence #860000, processed 14169936 words, keeping 162699 word types\n",
      "2018-03-05 16:24:07,697 : INFO : PROGRESS: at sentence #870000, processed 14337881 words, keeping 163810 word types\n",
      "2018-03-05 16:24:07,776 : INFO : PROGRESS: at sentence #880000, processed 14505288 words, keeping 164970 word types\n",
      "2018-03-05 16:24:07,836 : INFO : PROGRESS: at sentence #890000, processed 14669847 words, keeping 166206 word types\n",
      "2018-03-05 16:24:07,900 : INFO : PROGRESS: at sentence #900000, processed 14838070 words, keeping 167417 word types\n",
      "2018-03-05 16:24:07,972 : INFO : PROGRESS: at sentence #910000, processed 15001430 words, keeping 168728 word types\n",
      "2018-03-05 16:24:08,016 : INFO : PROGRESS: at sentence #920000, processed 15163511 words, keeping 169949 word types\n",
      "2018-03-05 16:24:08,088 : INFO : PROGRESS: at sentence #930000, processed 15329286 words, keeping 171109 word types\n",
      "2018-03-05 16:24:08,146 : INFO : PROGRESS: at sentence #940000, processed 15493363 words, keeping 172241 word types\n",
      "2018-03-05 16:24:08,155 : INFO : collected 172447 word types from a corpus of 15524316 raw words and 941911 sentences\n",
      "2018-03-05 16:24:08,156 : INFO : Loading a fresh vocabulary\n",
      "2018-03-05 16:24:08,270 : INFO : min_count=100 retains 7154 unique words (4% of original 172447, drops 165293)\n",
      "2018-03-05 16:24:08,271 : INFO : min_count=100 leaves 14717081 word corpus (94% of original 15524316, drops 807235)\n",
      "2018-03-05 16:24:08,304 : INFO : deleting the raw counts dictionary of 172447 items\n",
      "2018-03-05 16:24:08,313 : INFO : sample=0.001 downsamples 53 most-common words\n",
      "2018-03-05 16:24:08,315 : INFO : downsampling leaves estimated 10147719 word corpus (69.0% of prior 14717081)\n",
      "2018-03-05 16:24:08,364 : INFO : estimated required memory for 7154 words and 50 dimensions: 6438600 bytes\n",
      "2018-03-05 16:24:08,365 : INFO : resetting layer weights\n",
      "2018-03-05 16:24:08,503 : INFO : training model with 3 workers on 7154 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-05 16:24:09,527 : INFO : EPOCH 1 - PROGRESS: at 8.29% examples, 762180 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:10,541 : INFO : EPOCH 1 - PROGRESS: at 16.30% examples, 768823 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:11,547 : INFO : EPOCH 1 - PROGRESS: at 23.76% examples, 774971 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:12,547 : INFO : EPOCH 1 - PROGRESS: at 31.39% examples, 775038 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:13,551 : INFO : EPOCH 1 - PROGRESS: at 39.02% examples, 773647 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:14,551 : INFO : EPOCH 1 - PROGRESS: at 46.55% examples, 773207 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:15,553 : INFO : EPOCH 1 - PROGRESS: at 54.16% examples, 774451 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:16,553 : INFO : EPOCH 1 - PROGRESS: at 61.59% examples, 774659 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:17,554 : INFO : EPOCH 1 - PROGRESS: at 69.03% examples, 773824 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:18,566 : INFO : EPOCH 1 - PROGRESS: at 76.59% examples, 773043 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:19,573 : INFO : EPOCH 1 - PROGRESS: at 84.35% examples, 774091 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:20,577 : INFO : EPOCH 1 - PROGRESS: at 92.10% examples, 775191 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:21,585 : INFO : EPOCH 1 - PROGRESS: at 99.73% examples, 774752 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:21,595 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-05 16:24:21,607 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-05 16:24:21,616 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-05 16:24:21,618 : INFO : EPOCH - 1 : training on 15524316 raw words (10146175 effective words) took 13.1s, 774852 effective words/s\n",
      "2018-03-05 16:24:22,641 : INFO : EPOCH 2 - PROGRESS: at 8.08% examples, 741765 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:23,642 : INFO : EPOCH 2 - PROGRESS: at 16.06% examples, 760368 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:24,648 : INFO : EPOCH 2 - PROGRESS: at 23.31% examples, 762778 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:25,659 : INFO : EPOCH 2 - PROGRESS: at 30.94% examples, 764335 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:26,670 : INFO : EPOCH 2 - PROGRESS: at 38.64% examples, 765408 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:27,673 : INFO : EPOCH 2 - PROGRESS: at 46.23% examples, 766937 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:28,681 : INFO : EPOCH 2 - PROGRESS: at 53.72% examples, 766477 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:29,685 : INFO : EPOCH 2 - PROGRESS: at 61.21% examples, 768133 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:30,694 : INFO : EPOCH 2 - PROGRESS: at 68.71% examples, 768301 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:31,695 : INFO : EPOCH 2 - PROGRESS: at 76.39% examples, 770094 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:32,706 : INFO : EPOCH 2 - PROGRESS: at 84.15% examples, 771142 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:33,720 : INFO : EPOCH 2 - PROGRESS: at 91.91% examples, 771902 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:34,728 : INFO : EPOCH 2 - PROGRESS: at 99.67% examples, 772669 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:34,764 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-05 16:24:34,768 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-05 16:24:34,781 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-05 16:24:34,783 : INFO : EPOCH - 2 : training on 15524316 raw words (10149732 effective words) took 13.1s, 772008 effective words/s\n",
      "2018-03-05 16:24:35,800 : INFO : EPOCH 3 - PROGRESS: at 8.29% examples, 765105 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:36,806 : INFO : EPOCH 3 - PROGRESS: at 16.18% examples, 766898 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:37,818 : INFO : EPOCH 3 - PROGRESS: at 23.63% examples, 772298 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:38,824 : INFO : EPOCH 3 - PROGRESS: at 31.20% examples, 770780 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:39,841 : INFO : EPOCH 3 - PROGRESS: at 38.96% examples, 770991 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:40,842 : INFO : EPOCH 3 - PROGRESS: at 46.55% examples, 771903 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-05 16:24:41,847 : INFO : EPOCH 3 - PROGRESS: at 54.16% examples, 772924 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:42,848 : INFO : EPOCH 3 - PROGRESS: at 61.66% examples, 774039 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:43,852 : INFO : EPOCH 3 - PROGRESS: at 69.21% examples, 774558 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:44,853 : INFO : EPOCH 3 - PROGRESS: at 76.71% examples, 773884 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:45,858 : INFO : EPOCH 3 - PROGRESS: at 84.48% examples, 774984 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:46,865 : INFO : EPOCH 3 - PROGRESS: at 92.03% examples, 774204 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:47,866 : INFO : EPOCH 3 - PROGRESS: at 99.67% examples, 774278 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:47,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-05 16:24:47,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-05 16:24:47,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-05 16:24:47,911 : INFO : EPOCH - 3 : training on 15524316 raw words (10149829 effective words) took 13.1s, 774165 effective words/s\n",
      "2018-03-05 16:24:48,932 : INFO : EPOCH 4 - PROGRESS: at 8.36% examples, 771110 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:49,937 : INFO : EPOCH 4 - PROGRESS: at 16.30% examples, 773438 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:50,945 : INFO : EPOCH 4 - PROGRESS: at 23.57% examples, 771076 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:51,959 : INFO : EPOCH 4 - PROGRESS: at 31.26% examples, 771452 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:52,959 : INFO : EPOCH 4 - PROGRESS: at 38.90% examples, 771485 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:53,966 : INFO : EPOCH 4 - PROGRESS: at 46.61% examples, 773789 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:54,972 : INFO : EPOCH 4 - PROGRESS: at 53.72% examples, 766983 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:55,980 : INFO : EPOCH 4 - PROGRESS: at 60.46% examples, 758433 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:56,989 : INFO : EPOCH 4 - PROGRESS: at 67.63% examples, 756049 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:57,995 : INFO : EPOCH 4 - PROGRESS: at 75.23% examples, 758022 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:58,997 : INFO : EPOCH 4 - PROGRESS: at 82.81% examples, 758924 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:24:59,997 : INFO : EPOCH 4 - PROGRESS: at 90.44% examples, 760479 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:00,998 : INFO : EPOCH 4 - PROGRESS: at 97.94% examples, 760555 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:01,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-05 16:25:01,250 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-05 16:25:01,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-05 16:25:01,255 : INFO : EPOCH - 4 : training on 15524316 raw words (10146764 effective words) took 13.3s, 761589 effective words/s\n",
      "2018-03-05 16:25:02,278 : INFO : EPOCH 5 - PROGRESS: at 8.43% examples, 772764 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:03,287 : INFO : EPOCH 5 - PROGRESS: at 16.37% examples, 772635 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:04,300 : INFO : EPOCH 5 - PROGRESS: at 23.76% examples, 773787 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:05,301 : INFO : EPOCH 5 - PROGRESS: at 31.26% examples, 771100 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:06,304 : INFO : EPOCH 5 - PROGRESS: at 38.96% examples, 771944 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:07,311 : INFO : EPOCH 5 - PROGRESS: at 46.55% examples, 772111 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:08,320 : INFO : EPOCH 5 - PROGRESS: at 54.10% examples, 771640 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:09,331 : INFO : EPOCH 5 - PROGRESS: at 61.72% examples, 773599 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:10,336 : INFO : EPOCH 5 - PROGRESS: at 69.34% examples, 774695 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:11,339 : INFO : EPOCH 5 - PROGRESS: at 76.91% examples, 774425 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:12,343 : INFO : EPOCH 5 - PROGRESS: at 84.48% examples, 773826 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-05 16:25:13,362 : INFO : EPOCH 5 - PROGRESS: at 92.16% examples, 773425 words/s, in_qsize 6, out_qsize 0\n",
      "2018-03-05 16:25:14,374 : INFO : EPOCH 5 - PROGRESS: at 99.86% examples, 773450 words/s, in_qsize 3, out_qsize 0\n",
      "2018-03-05 16:25:14,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-05 16:25:14,377 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-05 16:25:14,388 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-05 16:25:14,389 : INFO : EPOCH - 5 : training on 15524316 raw words (10148300 effective words) took 13.1s, 773584 effective words/s\n",
      "2018-03-05 16:25:14,390 : INFO : training on a 77621580 raw words (50740800 effective words) took 65.9s, 770129 effective words/s\n",
      "2018-03-05 16:25:14,414 : INFO : saving Word2Vec object under word2vec_queries.m, separately None\n",
      "2018-03-05 16:25:14,416 : INFO : not storing attribute vectors_norm\n",
      "2018-03-05 16:25:14,420 : INFO : not storing attribute cum_table\n",
      "2018-03-05 16:25:14,498 : INFO : saved word2vec_queries.m\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(titles, size=50, window=10, min_count=100)\n",
    "model.save('word2vec_queries.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-05 16:34:16,379 : INFO : loading Word2Vec object from word2vec_queries.m\n",
      "2018-03-05 16:34:16,438 : INFO : loading wv recursively from word2vec_queries.m.wv.* with mmap=None\n",
      "2018-03-05 16:34:16,442 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-03-05 16:34:16,444 : INFO : loading vocabulary recursively from word2vec_queries.m.vocabulary.* with mmap=None\n",
      "2018-03-05 16:34:16,451 : INFO : loading trainables recursively from word2vec_queries.m.trainables.* with mmap=None\n",
      "2018-03-05 16:34:16,454 : INFO : setting ignored attribute cum_table to None\n",
      "2018-03-05 16:34:16,457 : INFO : loaded word2vec_queries.m\n",
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load('word2vec_queries.m')\n",
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(X_tsne, open('X_tsne_simple.p', 'wb'))\n",
    "pickle.dump(tsne, open('tsne_simple.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('tsne_query.csv', 'w+', encoding='utf-8', newline='') as outfile:\n",
    "    fieldnames = ['word', 'x', 'y']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for label, x, y in zip(vocab, X_tsne[:, 0], X_tsne[:, 1]):\n",
    "        writer.writerow({'word':label, 'x':x, 'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('warming', 0.6999518871307373),\n",
       " ('weather', 0.675580620765686),\n",
       " ('global', 0.6647760272026062),\n",
       " ('drought', 0.6540683507919312),\n",
       " ('biodiversity', 0.6473275423049927),\n",
       " ('anthropogenic', 0.6041305065155029),\n",
       " ('tides', 0.593826174736023),\n",
       " ('economy', 0.5922602415084839),\n",
       " ('greenhouse', 0.5857096910476685),\n",
       " ('dramatic', 0.5754234790802002)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['climate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
